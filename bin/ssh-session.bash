#!/bin/bash

# Use the provided container image path (optional).
CONTAINER_IMAGE=$2
USE_CONTAINER=true

# If no container image is specified, run without container
if [ -z "$CONTAINER_IMAGE" ]; then
    echo "‚ÑπÔ∏è No container specified, running directly on compute node"
    USE_CONTAINER=false
fi

# Handle container validation and setup only if using container
if [ "$USE_CONTAINER" = true ]; then
    # Handle /sc/projects paths by copying to home directory if needed
    if [[ "$CONTAINER_IMAGE" =~ ^/sc/projects ]]; then
        # Extract filename from the path
        SQSH_FILENAME=$(basename "$CONTAINER_IMAGE")
        LOCAL_SQSH_PATH="$HOME/$SQSH_FILENAME"
        
        # If local copy doesn't exist, try to copy from /sc/projects
        if [ ! -f "$LOCAL_SQSH_PATH" ]; then
            echo "Copying $CONTAINER_IMAGE to $LOCAL_SQSH_PATH..."
            if cp "$CONTAINER_IMAGE" "$LOCAL_SQSH_PATH" 2>/dev/null; then
                echo "‚úÖ Successfully copied sqsh file to home directory"
                CONTAINER_IMAGE="$LOCAL_SQSH_PATH"
            else
                echo "‚ö†Ô∏è Could not copy from /sc/projects, will try to use original path in container"
            fi
        else
            echo "‚ÑπÔ∏è Using existing sqsh file: $LOCAL_SQSH_PATH"
            CONTAINER_IMAGE="$LOCAL_SQSH_PATH"
        fi
    else
        # Check if the container image exists for non-/sc/projects paths
        if [ ! -f "$CONTAINER_IMAGE" ]; then
            echo "Error: Container image not found at '$CONTAINER_IMAGE'" >&2
            exit 1
        fi
    fi
fi

# Define the marker string to check if already added
MARKER="# >>> Slurm-over-SSH (auto-added) <<<"

# Check if SSH-over-SSH wrapper functions exist in bashrc
if grep -Fxq "$MARKER" "$HOME/.bashrc"; then
    # Remove SSH-over-SSH wrapper functions when NOT using containers
    if [ "$USE_CONTAINER" = false ]; then
        # Remove the SSH-over-SSH block from bashrc
        sed -i '/# >>> Slurm-over-SSH (auto-added) <<</,/# <<< Slurm-over-SSH (auto-added) >>>/d' "$HOME/.bashrc"
        echo "üóëÔ∏è  Removed SSH-over-SSH wrappers (using direct Slurm commands)"
    else
        echo "‚ÑπÔ∏è SSH-over-SSH wrappers already present (container mode)"
    fi
else
    # Only add SSH-over-SSH wrapper functions when using containers
    if [ "$USE_CONTAINER" = true ]; then
        cat >> "$HOME/.bashrc" <<'EOF'

# >>> Slurm-over-SSH (auto-added) <<<
if ! command -v sinfo >/dev/null 2>&1; then
  export SLURM_LOGIN=10.130.0.6

  sinfo()  { ssh -q "$SLURM_LOGIN" sinfo  "$@"; }
  squeue() { ssh -q "$SLURM_LOGIN" squeue "$@"; }
  sbatch() { ssh -q "$SLURM_LOGIN" sbatch "$@"; }
  srun() { ssh -q "$SLURM_LOGIN" srun "$@"; }
  salloc() { ssh -q "$SLURM_LOGIN" salloc "$@"; }
  scancel(){ ssh -q "$SLURM_LOGIN" scancel "$@"; }
fi
# <<< Slurm-over-SSH (auto-added) >>>
EOF

        echo "‚úÖ Added SSH-over-SSH wrappers (container mode)"
    else
        echo "‚ÑπÔ∏è Using direct Slurm commands (no wrappers needed)"
    fi
fi

# Add remote alias configuration
REMOTE_MARKER="# >>> Remote Alias Configuration (auto-added) <<<"
if ! grep -Fxq "$REMOTE_MARKER" "$HOME/.bashrc"; then
  cat >> "$HOME/.bashrc" <<'EOF'

# >>> Remote Alias Configuration (auto-added) <<<
# Define the path to start-ssh-job.bash
SSH_JOB_SCRIPT="${HOME}/bin/start-ssh-job.bash"

# Add remote alias if the script exists
if [ -f "$SSH_JOB_SCRIPT" ]; then
    alias remote="$SSH_JOB_SCRIPT"
fi

# Function to display remote options when entering slurm-cpu environment
display_slurm_options() {
    echo "üñ•Ô∏è  Welcome to the CPU environment!"
    echo "üìã Available 'remote' commands:"
    echo "   ‚Ä¢ remote list       - List running vscode-remote jobs"
    echo "   ‚Ä¢ remote ssh        - SSH into the node of a running job"
    echo "   ‚Ä¢ remote gpuswap    - Switch to GPU environment"
    echo "   ‚Ä¢ remote h100       - Reserve H100 GPU on aisc-shortrun partition"
    echo "   ‚Ä¢ remote exit       - Exit all jobs on aisc-interactive and aisc-shortrun partitions"
    echo "   ‚Ä¢ remote help       - Display full usage information"
    echo ""
    echo "üí° For GPU development:"
    echo "   ‚Ä¢ remote gpuswap    - Switch to GPU environment with salloc"
    echo "   ‚Ä¢ remote h100       - Reserve H100 GPU on aisc-shortrun partition"
    echo ""
    echo "üí° To return to local environment:"
    echo "   ‚Ä¢ remote exit       - Exit all interactive sessions completely"
    echo ""
}

# Add completion for remote commands (optional)
if command -v complete &>/dev/null; then
    _remote_completion() {
        local cur prev opts
        COMPREPLY=()
        cur="${COMP_WORDS[COMP_CWORD]}"
        prev="${COMP_WORDS[COMP_CWORD-1]}"
        
        opts="list ssh gpuswap h100 exit help"
        
        if [[ ${cur} == -* ]] ; then
            COMPREPLY=( $(compgen -W "-h --help" -- ${cur}) )
            return 0
        fi
        
        COMPREPLY=( $(compgen -W "${opts}" -- ${cur}) )
        return 0
    }
    
    complete -F _remote_completion remote
fi
# <<< Remote Alias Configuration (auto-added) >>>
EOF

  echo "‚úÖ Remote alias configuration added to ~/.bashrc"
else
  echo "‚ÑπÔ∏è Remote alias configuration already present"
fi

# Display remote options when entering the environment
if [ "$USE_CONTAINER" = true ]; then
    echo "üê≥ Container: $(basename "$CONTAINER_IMAGE")"
fi
display_slurm_options

if [ "$USE_CONTAINER" = true ]; then
    echo "üê≥ Starting SSH daemon in container: $(basename "$CONTAINER_IMAGE")"
    enroot start \
      --rw \
      --mount /usr/bin/srun:/usr/bin/srun \
      --mount /usr/bin/sbatch:/usr/bin/sbatch \
      --mount /usr/bin/scancel:/usr/bin/scancel \
      --mount /usr/lib/x86_64-linux-gnu/libslurm.so.41:/usr/lib/x86_64-linux-gnu/libslurm.so.41 \
      --mount /usr/lib/x86_64-linux-gnu/libslurm.so.41.0.0:/usr/lib/x86_64-linux-gnu/libslurm.so.41.0.0 \
      --mount /usr/lib/x86_64-linux-gnu/slurm-wlm:/usr/lib/x86_64-linux-gnu/slurm-wlm \
      --mount /usr/lib/x86_64-linux-gnu/libmunge.so.2:/usr/lib/x86_64-linux-gnu/libmunge.so.2 \
      "$CONTAINER_IMAGE" bash -c '
    if [ ! -d "${HOME:-~}/.ssh" ]; then
        mkdir -p ${HOME:-~}/.ssh
    fi

    if [ ! -f "${HOME:-~}/.ssh/vscode-remote-hostkey" ]; then
        ssh-keygen -t ed25519 -f ${HOME:-~}/.ssh/vscode-remote-hostkey -N ""
    fi

    if [ -f "/usr/sbin/sshd" ]; then
        sshd_cmd=/usr/sbin/sshd
    else
        sshd_cmd=sshd
    fi
    $sshd_cmd -D -p '$1' -f /dev/null -h ${HOME:-~}/.ssh/vscode-remote-hostkey
    '
else
    echo "üñ•Ô∏è  Starting SSH daemon directly on compute node"
    
    # Ensure SSH directory exists
    if [ ! -d "$HOME/.ssh" ]; then
        mkdir -p "$HOME/.ssh"
    fi

    # Generate SSH host key if it doesn't exist
    if [ ! -f "$HOME/.ssh/vscode-remote-hostkey" ]; then
        ssh-keygen -t ed25519 -f "$HOME/.ssh/vscode-remote-hostkey" -N ""
    fi

    # Find sshd binary
    if [ -f "/usr/sbin/sshd" ]; then
        sshd_cmd=/usr/sbin/sshd
    else
        sshd_cmd=sshd
    fi

    # Start SSH daemon directly on the compute node
    exec $sshd_cmd -D -p $1 -f /dev/null -h "$HOME/.ssh/vscode-remote-hostkey"
fi

